{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 903
    },
    "colab_type": "code",
    "id": "ZLYhgPAoIl72",
    "outputId": "2213aa3e-237a-4a8c-c05b-5be3ca67bae3"
   },
   "outputs": [],
   "source": [
    "# in google colab uncomment this\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/dmitrySorokin/Practical_RL/spring19/week06_policy_based/env_batch.py\n",
    "#!pip3 install gym==0.12.1\n",
    "#!pip3 install -e git+https://dmsoroki@bitbucket.org/dmsoroki/gym_interf.git@master#egg=gym_interf\n",
    "\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "#import os\n",
    "#if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "#    !bash ../xvfb start\n",
    "#    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3li8ux2rPNlJ",
    "outputId": "ef986fd2-6627-4422-ea9e-506a37fabb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\n",
      "gym (0.13.1)\n",
      "gym-interf (0.1, /home/dmitry/dev/gym_interf)\n",
      "pybulletgym (0.1, /home/dmitry/dev/pkgs/lib/pybullet-gym)\n"
     ]
    }
   ],
   "source": [
    "!pip3 list | grep gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epnPVTlwIl8S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "Process Process-6:\n",
      "Process Process-5:\n",
      "Process Process-7:\n",
      "Process Process-3:\n",
      "Process Process-4:\n",
      "Process Process-2:\n",
      "Process Process-8:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dmitry/dev/agents/a2c/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/home/dmitry/dev/agents/a2c/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dmitry/dev/agents/a2c/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/home/dmitry/dev/agents/a2c/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/home/dmitry/dev/agents/a2c/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dmitry/dev/agents/a2c/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/dmitry/dev/agents/a2c/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dmitry/dev/agents/a2c/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_interf\n",
    "from env_batch import ParallelEnvBatch\n",
    "import numpy as np\n",
    "\n",
    "def make_interf_env(seed):\n",
    "    env = gym.make('interf-v1')\n",
    "    env.set_calc_reward('delta_visib')\n",
    "    #env.set_calc_image('gpu')\n",
    "    env.seed(seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_env(nenvs):\n",
    "    seed = list(range(nenvs))\n",
    "    env = ParallelEnvBatch([\n",
    "        lambda env_seed=env_seed: make_interf_env(seed=env_seed)\n",
    "        for env_seed in seed\n",
    "    ])\n",
    "    return env\n",
    "\n",
    "\n",
    "N_ENVS = 8\n",
    "N_STEPS = 20\n",
    "env = make_env(nenvs=N_ENVS)\n",
    "obs = env.reset()\n",
    "N_ACTIONS = env.action_space.n\n",
    "OBS_SHAPE = obs.shape\n",
    "assert obs.shape == (8, 16, 64, 64), obs.shape\n",
    "assert obs.dtype == np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1059
    },
    "colab_type": "code",
    "id": "qrCdOUiFzjnM",
    "outputId": "20fb0517-9a2a-42b9-9d96-5201a0840e26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8), 0.8069883584112533),\n",
       " 0.017863257447654046,\n",
       " False,\n",
       " {'mirror1_normal': array([-1.11072771e-04,  7.07062351e-01, -7.07151200e-01]),\n",
       "  'mirror2_normal': array([-3.48948691e-09, -7.07062351e-01,  7.07151209e-01]),\n",
       "  'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [-1.57090486e-04  9.99999980e-01 -1.25639030e-04]',\n",
       "  'reflect_with_mirror2': 'center = [ -0.15707075  -0.12563903 -10.12562325], k = [-1.57095422e-04  2.46757156e-08  9.99999988e-01]',\n",
       "  'angle_between_beams': 0.000157095423319282,\n",
       "  'state_calc_time': 0.0008745193481445312,\n",
       "  'fit_time': 0,\n",
       "  'imin': 38.80729361630281,\n",
       "  'imax': 363.31657100519163,\n",
       "  'proj_1': array([0., 0., 0.]),\n",
       "  'proj_2': array([-0.15866144, -0.12563878,  0.        ]),\n",
       "  'dist': 0.20238220557125236,\n",
       "  'visib': 0.8069883584112533})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = make_interf_env(0)\n",
    "s = e.reset()\n",
    "e.step(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYMzWQLgIl8g"
   },
   "source": [
    "Next, we will need to implement a model that predicts logits and values. It is suggested that you use the same model as in [Nature DQN paper](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) with a modification that instead of having a single output layer, it will have two output layers taking as input the output of the last hidden layer. **Note** that this model is different from the model you used in homework where you implemented DQN. You can use your favorite deep learning framework here. We suggest that you use orthogonal initialization with parameter $\\sqrt{2}$ for kernels and initialize biases with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "67TMfsPOIl8l",
    "outputId": "b0c2dbfe-65b8-45f3-dbca-e0137fe35cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "\n",
    "def ortho_weights(shape, scale=1.):\n",
    "    \"\"\" PyTorch port of ortho_init from baselines.a2c.utils \"\"\"\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        flat_shape = shape[1], shape[0]\n",
    "    elif len(shape) == 4:\n",
    "        flat_shape = (np.prod(shape[1:]), shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    a = np.random.normal(0., 1., flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.transpose().copy().reshape(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        return torch.from_numpy((scale * q).astype(np.float32))\n",
    "    if len(shape) == 4:\n",
    "        return torch.from_numpy((scale * q[:, :shape[1], :shape[2]]).astype(np.float32))\n",
    "\n",
    "\n",
    "def atari_initializer(module):\n",
    "    \"\"\" Parameter initializer for Atari models\n",
    "    Initializes Linear, Conv2d, and LSTM weights.\n",
    "    \"\"\"\n",
    "    classname = module.__class__.__name__\n",
    "\n",
    "    if classname == 'Linear':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'Conv2d':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'LSTM':\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'weight_hh' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'bias' in name:\n",
    "                param.data.zero_()\n",
    "                \n",
    " \n",
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class AtariCNN(nn.Module):\n",
    "    def __init__(self, num_actions, state_shape):\n",
    "        \"\"\" Basic convolutional actor-critic network for Atari 2600 games\n",
    "        Equivalent to the network in the original DQN paper.\n",
    "        Args:\n",
    "            num_actions (int): the number of available discrete actions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels=state_shape[1], out_channels=32, kernel_size=8, stride=4),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "                                  nn.ReLU(),\n",
    "                                  Flatten())\n",
    "        \n",
    "        convw = conv2d_size_out(state_shape[2], kernel_size=8, stride=4)\n",
    "        convw = conv2d_size_out(convw, kernel_size=4, stride=2)\n",
    "        convw = conv2d_size_out(convw, kernel_size=3, stride=1)\n",
    "        \n",
    "        convh = conv2d_size_out(state_shape[3], kernel_size=8, stride=4)\n",
    "        convh = conv2d_size_out(convh, kernel_size=4, stride=2)\n",
    "        convh = conv2d_size_out(convh, kernel_size=3, stride=1)\n",
    "       \n",
    "        linear_input_size = convw * convh * 64\n",
    "        \n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(linear_input_size, 512),\n",
    "                                nn.ReLU())\n",
    "\n",
    "        self.pi = nn.Linear(512, num_actions)\n",
    "        self.v = nn.Linear(512, 1)\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # parameter initialization\n",
    "        self.apply(atari_initializer)\n",
    "        self.pi.weight.data = ortho_weights(self.pi.weight.size(), scale=.01)\n",
    "        self.v.weight.data = ortho_weights(self.v.weight.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Module forward pass\n",
    "        Args:\n",
    "            x (Variable): convolutional input, shaped [batch_size x 4 x 84 x 84]\n",
    "        Returns:\n",
    "            pi (Variable): action probability logits, shaped [batch_size x self.num_actions]\n",
    "            v (Variable): value predictions, shaped [batch_size x 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        conv_out = self.conv(x)\n",
    "\n",
    "        fc_out = self.fc(conv_out)\n",
    "\n",
    "        logits = self.pi(fc_out)\n",
    "        value = self.v(fc_out)\n",
    "\n",
    "        return logits, value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xd-NtMhIl81"
   },
   "source": [
    "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a dictionary of all the arrays that are needed to interact with an environment and train the model.\n",
    " Note that actions must be an `np.ndarray` while the other\n",
    "tensors need to have the type determined by your deep learning framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "u5lMKc4pIl86",
    "outputId": "f462782e-d29e-4970-cf96-4d026d4899de"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class ObserwationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, state):\n",
    "        return torch.tensor(state).float().to(DEVICE) / 255.0\n",
    "\n",
    "class Wrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "      return \n",
    "\n",
    "\n",
    "env = ObserwationWrapper(env)\n",
    "s = env.reset()\n",
    "print(s.shape)\n",
    "#s = s.transpose(0, 3, 1, 2)\n",
    "#s = torch.tensor(s)\n",
    "model = AtariCNN(N_ACTIONS, OBS_SHAPE).to(DEVICE)\n",
    "logits, values = model(s)\n",
    "print(logits)\n",
    "print(values)\n",
    "torch.distributions.Categorical(logits=logits).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "bd875-z-Il9M",
    "outputId": "5693b6d0-df57-4b58-efe4-38754741858a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "actions =  tensor([[0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 5],\n",
       "        [11],\n",
       "        [13]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(np.arange(16))\n",
    "a = a.reshape(4,4)\n",
    "print(a)\n",
    "a[:,3]\n",
    "actions = torch.tensor([0,1,3,1]).reshape(-1, 1)\n",
    "print('actions = ', actions)\n",
    "a.gather(1, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIlVt7s8Il9W"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class Policy:\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    \n",
    "  def act(self, inputs, determ = False):\n",
    "    logits, values = self.model(inputs)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    if determ:\n",
    "        actions = dist.probs.argmax(dim=1, keepdim=True)\n",
    "    else:\n",
    "        actions = dist.sample().view(-1, 1)\n",
    "\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    action_log_probs = log_probs.gather(1, actions)\n",
    "    \n",
    "    #print(actions.shape, logits.shape, action_log_probs.shape, values.shape)\n",
    "\n",
    "    return {\n",
    "        'actions': actions.reshape(-1).cpu().numpy(), \n",
    "        'logits': logits,\n",
    "        'log_probs': action_log_probs.reshape(-1), \n",
    "        'values': values.reshape(-1)\n",
    "    }\n",
    "    #<Implement policy by calling model, sampling actions and computing their log probs>\n",
    "    # Should return a dict containing keys ['actions', 'logits', 'log_probs', 'values']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "tyrAF9bgIl9f",
    "outputId": "6a2b0a94-34f9-4dbc-8cf2-222cf086f99f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': array([1, 5, 7, 3, 1, 0, 2, 6]),\n",
       " 'logits': tensor([[ 3.2460e-03,  2.2130e-04,  9.8520e-04,  2.4641e-04, -2.9515e-04,\n",
       "          -3.9923e-04,  3.2959e-03, -2.1542e-03],\n",
       "         [ 2.5310e-03,  1.5474e-03,  1.2582e-03,  2.7440e-04, -1.0643e-03,\n",
       "          -1.7829e-03,  2.7702e-03, -1.8419e-03],\n",
       "         [ 2.2369e-03,  7.3511e-04,  1.0906e-03,  5.9896e-04, -1.0082e-03,\n",
       "           3.4548e-04,  2.7260e-03, -1.3125e-03],\n",
       "         [ 3.3657e-03, -3.3464e-04,  5.3938e-04,  1.9687e-04,  1.2992e-04,\n",
       "           1.8506e-04,  3.4589e-03, -1.7094e-03],\n",
       "         [ 1.2336e-03,  4.9721e-05,  5.9780e-04,  7.1419e-04, -7.5546e-04,\n",
       "           1.2508e-03,  3.1766e-03, -9.2241e-04],\n",
       "         [ 2.4269e-03,  1.4068e-03,  9.2413e-04,  1.0750e-04, -8.7245e-04,\n",
       "          -1.2067e-03,  2.2483e-03, -1.4429e-03],\n",
       "         [ 2.6052e-03,  1.1304e-03,  8.9158e-04, -3.5806e-04, -1.2652e-03,\n",
       "          -1.2548e-03,  2.9650e-03, -3.0186e-03],\n",
       "         [ 2.3564e-03,  9.8325e-04,  1.2176e-03, -6.1996e-04, -1.1147e-03,\n",
       "          -9.6622e-04,  2.7079e-03, -1.9040e-03]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'log_probs': tensor([-2.0799, -2.0817, -2.0814, -2.0800, -2.0801, -2.0775, -2.0788, -2.0771],\n",
       "        device='cuda:0', grad_fn=<AsStridedBackward>),\n",
       " 'values': tensor([-0.1567, -0.1429, -0.1832, -0.0446, -0.1869, -0.1823, -0.1580, -0.1652],\n",
       "        device='cuda:0', grad_fn=<AsStridedBackward>)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AtariCNN(N_ACTIONS, OBS_SHAPE).to(DEVICE)\n",
    "policy = Policy(model)\n",
    "s = env.reset()       \n",
    "policy.act(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7e67LokIl9q"
   },
   "source": [
    "Next will pass the environment and policy to a runner that collects partial trajectories from the environment. \n",
    "The class that does is is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUJZSPT7Il9u"
   },
   "outputs": [],
   "source": [
    "\"\"\" RL env runner \"\"\"\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EnvRunner:\n",
    "  \"\"\" Reinforcement learning runner in an environment with given policy \"\"\"\n",
    "  def __init__(self, env, policy, nsteps,\n",
    "               transforms=None, step_var=None):\n",
    "    self.env = env\n",
    "    self.policy = policy\n",
    "    self.nsteps = nsteps\n",
    "    self.transforms = transforms or []\n",
    "    self.step_var = step_var if step_var is not None else 0\n",
    "    self.state = {\"latest_observation\": self.env.reset()}\n",
    "\n",
    "  @property\n",
    "  def nenvs(self):\n",
    "    \"\"\" Returns number of batched envs or `None` if env is not batched \"\"\"\n",
    "    return getattr(self.env.unwrapped, \"nenvs\", None)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\" Resets env and runner states. \"\"\"\n",
    "    self.state[\"latest_observation\"] = self.env.reset()\n",
    "\n",
    "  def get_next(self):\n",
    "    \"\"\" Runs the agent in the environment.  \"\"\"\n",
    "    trajectory = defaultdict(list, {\"actions\": []})\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    resets = []\n",
    "    self.state[\"env_steps\"] = self.nsteps\n",
    "\n",
    "    for i in range(self.nsteps):\n",
    "      observations.append(self.state[\"latest_observation\"])\n",
    "      act = self.policy.act(self.state[\"latest_observation\"])\n",
    "      if \"actions\" not in act:\n",
    "        raise ValueError(\"result of policy.act must contain 'actions' \"\n",
    "                         f\"but has keys {list(act.keys())}\")\n",
    "      for key, val in act.items():\n",
    "        trajectory[key].append(val)\n",
    "\n",
    "      obs, rew, done, _ = self.env.step(trajectory[\"actions\"][-1])\n",
    "      self.state[\"latest_observation\"] = obs\n",
    "      rewards.append(rew)\n",
    "      resets.append(done)\n",
    "      self.step_var += self.nenvs or 1\n",
    "\n",
    "      # Only reset if the env is not batched. Batched envs should auto-reset.\n",
    "      if not self.nenvs and np.all(done):\n",
    "        self.state[\"env_steps\"] = i + 1\n",
    "        self.state[\"latest_observation\"] = self.env.reset()\n",
    "\n",
    "    trajectory.update(observations=observations, rewards=rewards, resets=resets)\n",
    "    trajectory[\"state\"] = self.state\n",
    "\n",
    "    for transform in self.transforms:\n",
    "      transform(trajectory)\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYpcA3Z6Il-D"
   },
   "source": [
    "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
    "keys \n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* all other keys that you defined in `Policy`\n",
    "\n",
    "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGX69Q2DIl-K"
   },
   "source": [
    "To train the part of the model that predicts state values you will need to compute the value targets. \n",
    "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
    "Thus, we can implement and use `ComputeValueTargets` callable. \n",
    "The formula for the value targets is simple:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "In implementation, however, do not forget to use \n",
    "`trajectory['resets']` flags to check if you need to add the value targets at the next step when \n",
    "computing value targets for the current step. You can access `trajectory['state']['latest_observation']`\n",
    "to get last observations in partial trajectory &mdash; $s_{t+T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWyIxmtMIl-M"
   },
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "  def __init__(self, policy, gamma=0.99):\n",
    "    self.policy = policy\n",
    "    self.gamma = gamma\n",
    "    \n",
    "  def __call__(self, trajectory):\n",
    "    value_target = policy.act(trajectory['state']['latest_observation'])['values']\n",
    "    env_steps = trajectory['state']['env_steps']\n",
    "    rewards = torch.tensor(trajectory['rewards'], dtype=torch.float).to(DEVICE)\n",
    "    dones = torch.tensor(trajectory['resets'], dtype=torch.float).to(DEVICE)\n",
    "    is_not_done = 1 - dones\n",
    "\n",
    "    trajectory['value_targets'] = [0] * env_steps\n",
    "    for i in range(env_steps):\n",
    "        j = env_steps - i - 1\n",
    "        value_target = rewards[j] + value_target * self.gamma * is_not_done[j]\n",
    "        trajectory['value_targets'][j] = value_target\n",
    "\n",
    "\n",
    "    # This method should modify trajectory inplace by adding \n",
    "    # an item with key 'value_targets' to it. \n",
    "    #<Compute value targets for a given partial trajectory>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fh_Vq2IvIl-S"
   },
   "source": [
    "After computing value targets we will transform lists of interactions into tensors\n",
    "with the first dimension `batch_size` which is equal to `T * nenvs`, i.e. you essentially need\n",
    "to flatten the first two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5N5ev_tRIl-U"
   },
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "  \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
    "  def __call__(self, trajectory):\n",
    "    # Modify trajectory inplace. \n",
    "    #<TODO: implement>\n",
    "    #'actions', 'logits', 'log_probs', 'values', 'observations', 'rewards', 'resets', 'state', 'value_targets'\n",
    "        \n",
    "    trajectory['value_targets'] = torch.stack(trajectory['value_targets'])\n",
    "    trajectory['value_targets'] = trajectory['value_targets'].flatten(0, 1)\n",
    "    trajectory['values'] = torch.stack(trajectory['values']).flatten(0, 1)\n",
    "    trajectory['logits'] = torch.stack(trajectory['logits']).flatten(0, 1)\n",
    "    trajectory['log_probs'] = torch.stack(trajectory['log_probs']).flatten(0, 1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdTaYTIYIl-c"
   },
   "outputs": [],
   "source": [
    "model = AtariCNN(N_ACTIONS, OBS_SHAPE).to(DEVICE)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=N_STEPS,\n",
    "                   transforms=[ComputeValueTargets(policy),\n",
    "                               MergeTimeBatch()])\n",
    "\n",
    "\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3uYsxqbIl-m"
   },
   "source": [
    "Now is the time to implement the advantage actor critic algorithm itself. You can look into your lecture,\n",
    "[Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and [lecture](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) by Sergey Levine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GgL-VfDIl-q"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class A2C:\n",
    "  def __init__(self,\n",
    "               policy,\n",
    "               optimizer,\n",
    "               value_loss_coef=0.25,\n",
    "               entropy_coef=0.01,\n",
    "               max_grad_norm=0.5):\n",
    "    self.policy = policy\n",
    "    self.optimizer = optimizer\n",
    "    self.value_loss_coef = value_loss_coef\n",
    "    self.entropy_coef = entropy_coef\n",
    "    self.max_grad_norm = max_grad_norm\n",
    "    \n",
    "    self.ploss = None\n",
    "    self.vloss = None\n",
    "    self.a2c_loss = None\n",
    "    \n",
    "    self.vtargets = None\n",
    "    self.values = None\n",
    "    self.advantage = None\n",
    "    self.entropy = None\n",
    "    \n",
    "    self.grad_norm = None\n",
    "    \n",
    "  def policy_loss(self, trajectory):\n",
    "    advantage = trajectory['value_targets'] - trajectory['values']\n",
    "    \n",
    "    logits = trajectory['logits']\n",
    "    probs = nn.functional.softmax(logits, dim=-1)\n",
    "    logprobs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    entropy = -torch.sum(probs * logprobs, dim=-1)\n",
    "\n",
    "    actor_loss = -torch.mean(trajectory['log_probs'] * advantage.detach()) - self.entropy_coef * torch.mean(entropy)\n",
    "    \n",
    "    self.ploss = actor_loss.item()\n",
    "    self.vtargets = torch.mean(trajectory['value_targets']).item()\n",
    "    self.values = torch.mean(trajectory['values']).item()\n",
    "    self.advantage = torch.mean(advantage).item()\n",
    "    self.entropy = torch.mean(entropy).item()\n",
    "    \n",
    "    return actor_loss\n",
    " \n",
    "  def value_loss(self, trajectory):\n",
    "    critic_loss = torch.mean((trajectory['values'] - trajectory['value_targets'].detach())**2)\n",
    "    self.vloss = critic_loss.item()\n",
    "    return critic_loss\n",
    "    \n",
    "  def loss(self, trajectory):\n",
    "    total_loss = self.policy_loss(trajectory) + self.value_loss_coef * self.value_loss(trajectory)\n",
    "    self.a2c_loss = total_loss.item()\n",
    "    return total_loss\n",
    "      \n",
    "  def step(self, trajectory):\n",
    "    self.optimizer.zero_grad()\n",
    "    self.loss(trajectory).backward()\n",
    "    self.grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), self.max_grad_norm)\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-inAO_NIl-u"
   },
   "source": [
    "Now you can train your model. With reasonable hyperparameters training on a single GTX1080 for 10 million steps across all batched environments (which translates to about 5 hours of wall clock time)\n",
    "it should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last \n",
    "episodes in each environment in the batch) of about 600. You should plot this quantity with respect to \n",
    "`runner.step_var` &mdash; the number of interactions with all environments. It is highly \n",
    "encouraged to also provide plots of the following quantities (these are useful for debugging as well):\n",
    "\n",
    "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) between \n",
    "value targets and value predictions\n",
    "* Entropy of the policy $\\pi$\n",
    "* Value loss\n",
    "* Policy loss\n",
    "* Value targets\n",
    "* Value predictions\n",
    "* Gradient norm\n",
    "* Advantages\n",
    "* A2C loss\n",
    "\n",
    "For optimization we suggest you use RMSProp with learning rate starting from 7e-4 and linearly decayed to 0, smoothing constant (alpha in PyTorch and decay in TensorFlow) equal to 0.99 and epsilon equal to 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def evaluate(policy, env, n_games=10):\n",
    "    n_solved = 0\n",
    "    n_steps = 0\n",
    "    dist = 0\n",
    "    angle = 0\n",
    "    for i in range(n_games):\n",
    "        s = env.reset()\n",
    "        istep = 0\n",
    "        while(True):\n",
    "            s = s.reshape(1,16,64,64)\n",
    "            action = policy.act(s, determ=True)['actions'][0]\n",
    "            s, reward, done, info = env.step(action)\n",
    "            istep += 1\n",
    "            if done:\n",
    "                n_solved += istep < 200\n",
    "                n_steps += istep\n",
    "                dist += info['dist']\n",
    "                angle += info['angle_between_beams']\n",
    "                break\n",
    "    return n_solved / n_games, n_steps / n_games, dist / n_games, angle / n_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "colab_type": "code",
    "id": "uAynSgnKIl-0",
    "outputId": "1041d8b3-9cfc-4738-d20c-2c3578a1bd62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 28545/62500 [8:18:32<9:06:34,  1.04it/s] /home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:260: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      " 68%|██████▊   | 42275/62500 [12:13:12<5:47:44,  1.03s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b19122abd8b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_ENVS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0ma2c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-797085f73445>\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"latest_observation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"latest_observation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"actions\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         raise ValueError(\"result of policy.act must contain 'actions' \"\n",
      "\u001b[0;32m<ipython-input-8-d12ac5b32bc3>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, inputs, determ)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3d1228e367d9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mvisibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mvisib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisibs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\n",
    "    'runs/a2c_with_visib',\n",
    "    'fixed visibility calculation'\n",
    ")\n",
    "\n",
    "agent = AtariCNN(N_ACTIONS, OBS_SHAPE).to(DEVICE)\n",
    "opt = torch.optim.RMSprop(agent.parameters(), lr=7e-4, alpha=0.99, eps=1e-5)\n",
    "policy = Policy(agent)\n",
    "evaluate_env = make_interf_env(1234)\n",
    "a2c = A2C(policy, opt)\n",
    "runner = EnvRunner(env, policy, nsteps=N_STEPS, transforms=[ComputeValueTargets(policy), MergeTimeBatch()])\n",
    "runner.reset()\n",
    "\n",
    "entropis = []\n",
    "vlosses = []\n",
    "plosses = []\n",
    "vtargets = []\n",
    "vpredictions = []\n",
    "grad_norms = []\n",
    "advantages = []\n",
    "a2c_losses = []\n",
    "rewards = np.zeros(N_ENVS, dtype=float)\n",
    "dones = np.zeros(N_ENVS, dtype=float)\n",
    "steps = np.zeros(N_ENVS, dtype=float)\n",
    "\n",
    "eval_solved_games = []\n",
    "mean_eval_steps = []\n",
    "n_frames = []\n",
    "\n",
    "\n",
    "for i in trange(0, int(1e7), N_ENVS * N_STEPS):\n",
    "    trajectory = runner.get_next()\n",
    "    a2c.step(trajectory)\n",
    "    \n",
    "    for batch_rewards, batch_dones in zip(trajectory['rewards'], trajectory['resets']):\n",
    "        rewards += batch_rewards\n",
    "        dones += batch_dones\n",
    "        steps += 1\n",
    "    \n",
    "    entropis.append(a2c.entropy)\n",
    "    vlosses.append(a2c.vloss)\n",
    "    plosses.append(a2c.ploss)\n",
    "    vtargets.append(a2c.vtargets)\n",
    "    vpredictions.append(a2c.values)\n",
    "    grad_norms.append(a2c.grad_norm)\n",
    "    advantages.append(a2c.advantage)\n",
    "    a2c_losses.append(a2c.a2c_loss)\n",
    "        \n",
    "    if np.sum(dones) >= 100:\n",
    "        writer.add_scalar('entropy', np.mean(entropis), i)\n",
    "        writer.add_scalar('vloss', np.mean(vlosses), i)\n",
    "        writer.add_scalar('ploss', np.mean(plosses), i)\n",
    "        writer.add_scalar('vtarget', np.mean(vtargets), i)\n",
    "        writer.add_scalar('vprediction', np.mean(vpredictions), i)\n",
    "        writer.add_scalar('grad_norm', np.mean(grad_norms), i)\n",
    "        writer.add_scalar('advantage', np.mean(advantages), i)\n",
    "        writer.add_scalar('loss', np.mean(a2c_losses), i)\n",
    "        writer.add_scalar('entropy', np.mean(entropis), i)\n",
    "        writer.add_scalar('reward', np.mean(rewards / dones), i)\n",
    "        writer.add_scalar('steps', np.mean(steps / dones), i)\n",
    "        \n",
    "        n_solved, n_steps, dist, angle = evaluate(policy, evaluate_env)\n",
    "        \n",
    "        writer.add_scalar('eval_solved_games', n_solved, i)\n",
    "        writer.add_scalar('eval_steps', n_steps, i)\n",
    "        writer.add_scalar('dist_between_beams', dist, i)\n",
    "        writer.add_scalar('angle_between_beams', angle, i)\n",
    "\n",
    "        entropis = []\n",
    "        vlosses = []\n",
    "        plosses = []\n",
    "        vtargets = []\n",
    "        vpredictions = []\n",
    "        grad_norms = []\n",
    "        advantages = []\n",
    "        a2c_losses = []\n",
    "        rewards = np.zeros(N_ENVS, dtype=float)\n",
    "        dones = np.zeros(N_ENVS, dtype=float)\n",
    "        steps = np.zeros(N_ENVS, dtype=float)\n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "id": "e5u7-w02Il-8",
    "outputId": "e96d8ce9-642b-4807-ee19-b7a5e2afc479"
   },
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), 'a2c_interf_model_visib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfFCqTPjVn3H"
   },
   "outputs": [],
   "source": [
    "evaluate(policy, evaluate_env, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQqro-2WIl_K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ihVVgXu5gRek",
    "outputId": "ce90286c-d7d8-4ce3-c122-880ea0665d41"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW8cKjiTgUsu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "a2c_cuda.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
